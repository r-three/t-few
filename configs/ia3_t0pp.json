{
    "lora_scaling_rank": 1,
    "lora_rank": 0,
    "lora_init_scale": 0.0,
    "lora_modules": ".*SelfAttention|.*EncDecAttention|.*DenseReluDense",
    "lora_layers": "k|v|wi_1.*",
    "trainable_param_names": ".*lora_b.*",
    "model_modifier": "lora",
    "lr": 3e-3,
    "num_steps": 3000,
    "dataset": "adherence",
    "few_shot": false,
    "unlikely_loss": 1,
    "length_norm": 1,
    "mc_loss": 1,
    "batch_size": 1,
    "grad_accum_factor": 8,
    "save_model": true,
    "compute_strategy": "deepspeed_stage_3",
    "origin_model": "bigscience/T0",
    "load_weight": "pretrained_checkpoints/t011b_ia3_finish.pt",
    "eval_batch_size": 1
}